{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:Georgia; font-size:1em;\">Yelp Webscraping Project</span>\n",
    "\n",
    "\n",
    "## <span style=\"font-family:Georgia; font-size:1em;text-align: justify;\">Short Description: This project scrapes the business name, phone number, address, and the amount of reviews for each vape shop listed on Yelp in the United States and places them into a CSV file in my home directory. The vape shop search term is interchangable and can be switched out for any other key term for example resturants.</span>\n",
    "\n",
    "### <span style=\"font-family:Georgia; font-size:1em;text-align: justify; text-justify: inter-word;\">This project is the first version of many to come. It is definitely not as fast as it could be; it scraps around 10 zipcodes every ten minutes. On my Intel Core I5 processor running on 9.7 GB of Ram, I can use three terminals and scrape approximately 30 zipcodes per minute.</span>\n",
    "\n",
    "### <span style=\"font-family:Georgia; font-size:1em;text-align: justify;text-justify: inter-word;\">Essentially, there are many other options that I can use to optimize this program for example adding multithreading, or even allowing a user to enter a specific zipcode they would want scraped. The one main thing I need to work on is logging information. At this point in time, the program only prints which url is being scraped. However, I should be logging at key points within the program, for example when it opens the webpage, how many businesses were scrapped per zipcode, which functions are working appropriately, etc. For fun, I will be using Apache Spark to help analyze the data. in real time for the last %10 of the zipcodes in the United States.</span>\n",
    "\n",
    "### <span style=\"font-family:Georgia; font-size:.9em;text-align: justify;text-justify: inter-word;\">Things I plan to do in the near future:</span>\n",
    "\n",
    "<span style=\"font-family:Georgia; font-size:.9em;text-align: justify;text-justify: inter-word;\">\n",
    "\n",
    "1. Geocode the addresses + begin cleaning the data, specifically removing duplicates \n",
    "\n",
    "2. Use Tableau to display all of the locations of the Vape Shops\n",
    "\n",
    "3. Get the raw counts of vapor stores by state \n",
    "\n",
    "4. Adjust the counts according to population in these areas \n",
    "\n",
    "5. Evaluate the relationships (if there are any) between smoking death rates, tax standards for tobacco products, and newly imposed legislation patterns. \n",
    "\n",
    "6. In places with high concentrations of Vape Shops, number to be distinguished in the near future, I will also look at the socioeconomic tendencies around these high concentration areas</span>\n",
    "\n",
    "#### <span style=\"font-family:Georgia; font-size:.85em;text-align: justify; text-justify: inter-word;\">Disclosure: There are better scraping solutions out there, however, this was my first attempt at scarping. In general, scraping Yelp is looked down upon. Please take a look at their robots.txt. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named bs4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0924ac834afe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0m__date__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'April 19th, 2016'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named bs4"
     ]
    }
   ],
   "source": [
    "__author__ = 'Zachary Diemer'\n",
    "__date__ = 'April 19th, 2016'\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "#from urlparse import urljoin\n",
    "\n",
    "#Global Declarations\n",
    "ZIP_URL = \"zipcodes.txt\"\n",
    "path_to_chromedriver ='/home/zackymo/Desktop/chromedriver'\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver)\n",
    "\n",
    "#This funtion pulls the zipcodes from a text file, stores them into a variable.\n",
    "\n",
    "def get_zips():\n",
    "    f = open(ZIP_URL, 'r+')\n",
    "    zips = [zcs.strip() for zcs in f.read().split('\\n') if zcs.strip() ]\n",
    "    f.close()\n",
    "    return zips\n",
    "\n",
    "#This function modifies the current url \n",
    "\n",
    "get_yelp_page = \\\n",
    "    lambda zipcode: \\\n",
    "        'http://www.yelp.com/search?find_desc=vape+shops&find_loc={0}'.format(zipcode)\n",
    "\n",
    "#This synchronizes BeautifulSoup and Selenium\n",
    "#The first command transfers the entire page into html\n",
    "#and the second sets up bs\n",
    "def make_soup(url):\n",
    "    html = browser.page_source\n",
    "    return BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "\n",
    "#Creating, and opening the new csvFile, creating the header names, and set up the file for data entry \n",
    "csvFile = open('vapeshops.csv', 'w')\n",
    "fieldnames = ['bizname', 'addr','bizphone','numrevs']# 'cheapornah', 'rating', 'wifiavail']#,'cheapornah','rating', 'wifiavail']\n",
    "writer = csv.DictWriter(csvFile, fieldnames=fieldnames, delimiter=',', lineterminator='\\n')\n",
    "writer.writeheader()\n",
    "\n",
    "\n",
    "#This function scrapes the url for the specified information\n",
    "\n",
    "def gather_info(url):\n",
    "    soup = make_soup(url)\n",
    "    vapeshops = soup.find_all(\"div\", {\"class\": \"search-result\"})\n",
    "    for v in vapeshops:\n",
    "        desc = {}\n",
    "        try:\n",
    "            desc['bizname'] = v.contents[1].find_all(\"a\", {\"class\": \"biz-name\"})[0].text.encode(\"utf-8\")\n",
    "            print bizname\n",
    "        except: #Exception, e:\n",
    "            pass#if verbose: print 'Business name extract fail', str(e)\n",
    "        try:\n",
    "            desc['addr'] = v.contents[1].find_all(\"address\")[0].getText(separator=u', ').encode(\"utf-8\") #works fully with commas\n",
    "            print addr\n",
    "        except: #Exception, e:\n",
    "            pass#if verbose: print 'Business address extract fail', str(e)\n",
    "        try:\n",
    "            desc['bizphone'] = v.contents[1].find_all(\"span\", {\"class\": \"biz-phone\"})[0].text.encode(\"utf-8\") #works\n",
    "            print bizphone\n",
    "        except: #Exception, e:\n",
    "            pass#if verbose: print 'Phone number extract fail', str(e)\n",
    "        #print item.contents[1].find_all(\"div\", {\"class\": \"rating-large\"}) #sort of works\n",
    "        try:\n",
    "            desc['numrevs'] = v.contents[1].find_all(\"span\", {\"class\": \"review-count\"})[0].text.encode(\"utf-8\") #works\n",
    "            print numrevs\n",
    "        except: #Exception, e:\n",
    "            pass#if verbose: print 'Number of reviews extract fail', str(e)\n",
    "        \"\"\"try:\n",
    "            desc['cheapornah'] = v.contents[1].find_all(\"span\", {\"class\": \"price-range\"}).encode(\"utf-8\")\n",
    "            print cheapornah\n",
    "        except: #Exception, e:\n",
    "            pass#if verbose: print ' extract fail', str(e)\n",
    "        try:\n",
    "            desc['rating'] = v.contents[1].find_all(\"div\", {\"class\": \"rating-large\"}).encode(\"utf-8\")\n",
    "            print rating\n",
    "        except: #Exception, e:\n",
    "            pass#if verbose: print ' extract fail', str(e)\n",
    "        try:\n",
    "            desc['wifiavail'] = v.find('dd', {'class':'attr-WiFi'}).getText().encode(\"utf-8\")\n",
    "            print wifiavail\n",
    "        except: #Exception, e:\n",
    "            pass#if verbose: print 'Wifi availability extract fail', str(e)\"\"\"\n",
    "        writer.writerow(desc)\n",
    "\n",
    "        \n",
    "\n",
    "#This is essentially the main function. It executes the various functions and outputs the url of the zipcode\n",
    "#that is being scraped to the terminal\n",
    "\n",
    "def crawl():\n",
    "    zipcodes = get_zips()\n",
    "    for z in zipcodes:\n",
    "            #Add the zipcode to the Base URL\n",
    "            initial_url = get_yelp_page(z)\n",
    "            #Log that info somehow\n",
    "            print initial_url\n",
    "            #Use Selenium to display the URL\n",
    "            browser.get(initial_url)\n",
    "            #Gather the specific information you are looking for\n",
    "            gather_info(initial_url)\n",
    "            #Attempt to go to the next page\n",
    "            try:\n",
    "                browser.find_element_by_css_selector('span[class=\\\"pagination-label u-align-middle responsive-hidden-small pagination-links_anchor\\\"]').click()\n",
    "            except:\n",
    "                pass\n",
    "            #Create a new variable to update url over time\n",
    "            prev_url = initial_url\n",
    "            #Get the new page's url\n",
    "            new_url = browser.current_url\n",
    "            #counter = 0 - If I would like, I can output the number of businesses extracted in each zipcode\n",
    "            #Continue this process until you go through 7 different pages or refresh 10 different times\n",
    "            while prev_url != new_url: #and counter <= 10:\n",
    "                prev_url = new_url\n",
    "                gather_info(new_url)\n",
    "                browser.get(new_url)\n",
    "                try:\n",
    "                    browser.find_element_by_css_selector('span[class=\\\"pagination-label u-align-middle responsive-hidden-small pagination-links_anchor\\\"]').click()\n",
    "                except:\n",
    "                    pass\n",
    "                new_url = browser.current_url\n",
    "                #counter += 1\n",
    "            #Print counter or add to a text file with zipcode name here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
